{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amantimalsina/Mamba-SAE/blob/main/training_a_sparse_autoencoder_for_mamba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O8tQblzOVHu"
      },
      "source": [
        "# A very basic SAE Training Tutorial\n",
        "\n",
        "Please note that it is very easy for tutorial code to go stale so please have a low bar for raising an issue in the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shAFb9-lOVHu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sae-lens transformer-lens circuitsvis\n",
        "\n",
        "# MambaLens:\n",
        "!pip install git+https://github.com/Phylliida/MambaLens.git\n",
        "# For faster inference in SSMs:\n",
        "!pip install causal_conv1d mamba-ssm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9L-LZJFkmxNt",
        "outputId": "936438b4-3403-4590-fcab-f78172b8ebb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sae-lens in /usr/local/lib/python3.10/dist-packages (3.18.2)\n",
            "Requirement already satisfied: transformer-lens in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.10/dist-packages (1.43.2)\n",
            "Requirement already satisfied: automated-interpretability<1.0.0,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.0.6)\n",
            "Requirement already satisfied: babe<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.0.7)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (2.21.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (3.9.2)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.1.7)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (3.8.1)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.19.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (5.24.0)\n",
            "Requirement already satisfied: plotly-express<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.4.1)\n",
            "Requirement already satisfied: pytest-profiling<2.0.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (1.7.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (1.0.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (6.0.1)\n",
            "Requirement already satisfied: pyzmq==26.0.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (26.0.0)\n",
            "Requirement already satisfied: safetensors<0.5.0,>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.4.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.1 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (4.41.2)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (4.12.2)\n",
            "Requirement already satisfied: zstandard<0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from sae-lens) (0.22.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.33.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.2.33)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.0.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (2.1.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (4.66.4)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens) (0.17.8)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (8.0.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->circuitsvis) (12.6.68)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.1.0->circuitsvis) (3.15.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens) (0.23.4)\n",
            "Requirement already satisfied: blobfile<3.0.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (2.1.1)\n",
            "Requirement already satisfied: boostedblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.15.4)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (3.10.7)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (1.2.2)\n",
            "Requirement already satisfied: tiktoken<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae-lens) (0.6.0)\n",
            "Requirement already satisfied: py2store in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.20)\n",
            "Requirement already satisfied: graze in /usr/local/lib/python3.10/dist-packages (from babe<0.0.8,>=0.0.7->sae-lens) (0.1.24)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.17.1->sae-lens) (3.9.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.19.2)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens) (2.13.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (1.4.5)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae-lens) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline<0.2.0,>=0.1.6->sae-lens) (5.7.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 229, in _get_updated_criteria\n",
            "    for requirement in self._p.get_dependencies(candidate=candidate):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 244, in get_dependencies\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 244, in <listcomp>\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 391, in iter_dependencies\n",
            "    for r in self.dist.iter_dependencies():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in _compute_dependencies\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in <listcomp>\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3124, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 304, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 226, in _evaluate_markers\n",
            "    assert isinstance(marker, (list, tuple, str))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 168, in emit\n",
            "    message = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 679, in format\n",
            "    if self.usesTime():\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 647, in usesTime\n",
            "    return self._style.usesTime()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 424, in usesTime\n",
            "    return self._fmt.find(self.asctime_search) >= 0\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting git+https://github.com/Phylliida/MambaLens.git\n",
            "  Cloning https://github.com/Phylliida/MambaLens.git to /tmp/pip-req-build-45dhg7ce\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Phylliida/MambaLens.git /tmp/pip-req-build-45dhg7ce\n",
            "\u001b[31m  ERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: causal_conv1d in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal_conv1d) (2.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal_conv1d) (24.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from causal_conv1d) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d) (2.18.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 148, in _add_to_criteria\n",
            "    matches = self._p.find_matches(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 231, in find_matches\n",
            "    return self._factory.find_candidates(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 391, in find_candidates\n",
            "    parsed_requirement = get_requirement(identifier)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/packaging.py\", line 45, in get_requirement\n",
            "    return Requirement(req_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4108, in parseImpl\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 172, in emit\n",
            "    style = Style(color=\"red\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/style.py\", line 122, in __init__\n",
            "    def __init__(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "import gc\n",
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "# %%\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import pprint\n",
        "# %%\n",
        "import argparse\n",
        "# %%\n",
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "# %%\n",
        "from transformer_lens.utils import test_prompt\n",
        "import circuitsvis as cv  # optional dep, install with pip install circuitsvis\n",
        "from functools import partial\n",
        "# %%\n",
        "import mamba_lens"
      ],
      "metadata": {
        "id": "mHOdhs_lnCGg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy-b3CcSOVHu",
        "outputId": "0964d700-cdcb-47b0-f757-25bc89836d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe2nlqf-OVHv"
      },
      "source": [
        "# Model Selection and Evaluation (Feel Free to Skip)\n",
        "\n",
        "We'll use the runner to train an SAE on a TinyStories Model. This is a very small model so we can train an SAE on it quite quickly. Before we get started, let's load in the model with `transformer_lens` and see what it can do.\n",
        "\n",
        "TransformerLens gives us 2 functions that are useful here (and circuits viz provides a third):\n",
        "1. `transformer_lens.utils.test_prompt` will help us see when the model can infer one token.\n",
        "2. `HookedTransformer.generate` will help us see what happens when we sample from the model.\n",
        "3. `circuitsvis.logits.token_log_probs` will help us visualize the log probs of tokens at several positions in a prompt."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = mamba_lens.HookedMamba.from_pretrained(\n",
        "                              \"state-spaces/mamba-130m\",\n",
        "                              device='cuda'\n",
        "                              )\n",
        "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
      ],
      "metadata": {
        "id": "OO0grtwgzyTi",
        "outputId": "4ea5c276-07e5-446d-c029-614e087486e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving model to device:  cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_layers = model.cfg.n_layers\n",
        "d_model = model.cfg.d_model\n",
        "d_state = model.cfg.d_state\n",
        "d_conv = model.cfg.d_conv\n",
        "n_ctx = model.cfg.n_ctx\n",
        "d_inner = model.cfg.d_inner\n",
        "print(f\"n_layers: {n_layers}\")\n",
        "print(f\"d_model: {d_model}\")\n",
        "print(f\"d_state: {d_state}\")\n",
        "print(f\"d_conv: {d_conv}\")\n",
        "print(f\"n_ctx: {n_ctx}\")\n",
        "print(f\"d_inner: {d_inner}\")\n",
        "print(model.cfg)"
      ],
      "metadata": {
        "id": "ZqmAhJ71RtYl",
        "outputId": "7af36de5-50e7-43f4-fe40-5da17f76edce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_layers: 24\n",
            "d_model: 768\n",
            "d_state: 16\n",
            "d_conv: 4\n",
            "n_ctx: 2048\n",
            "d_inner: 1536\n",
            "MambaCfg(d_model=768, n_layers=24, vocab_size=50280, d_state=16, expand=2, dt_rank=48, d_conv=4, pad_vocab_size_multiple=8, conv_bias=True, bias=False, default_prepend_bos=True, tokenizer_prepends_bos=False, n_ctx=2048, device='cuda', initializer_cfg=MambaInitCfg(initializer_range=(0.02,), rescale_prenorm_residual=(True,), n_residuals_per_layer=(1,), dt_init=('random',), dt_scale=(1.0,), dt_min=(0.001,), dt_max=(0.1,), dt_init_floor=0.0001), d_inner=1536)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIWDRTgp_V6z",
        "outputId": "fa7be3f8-e1a0-422c-eac1-6f27420a7061"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HookedMamba(\n",
            "  (embedding): Embedding(50280, 768)\n",
            "  (hook_embed): HookPoint()\n",
            "  (blocks): ModuleList(\n",
            "    (0-23): 24 x HookedMambaBlock(\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_layer_input): HookPoint()\n",
            "      (norm): RMSNorm()\n",
            "      (hook_normalized_input): HookPoint()\n",
            "      (skip_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
            "      (hook_skip): HookPoint()\n",
            "      (in_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
            "      (hook_in_proj): HookPoint()\n",
            "      (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
            "      (hook_conv): HookPoint()\n",
            "      (hook_ssm_input): HookPoint()\n",
            "      (W_delta_1): Linear(in_features=1536, out_features=48, bias=False)\n",
            "      (W_delta_2): Linear(in_features=48, out_features=1536, bias=True)\n",
            "      (W_B): Linear(in_features=1536, out_features=16, bias=False)\n",
            "      (W_C): Linear(in_features=1536, out_features=16, bias=False)\n",
            "      (hook_h_start): HookPoint()\n",
            "      (hook_delta_1): HookPoint()\n",
            "      (hook_delta_2): HookPoint()\n",
            "      (hook_delta): HookPoint()\n",
            "      (hook_A): HookPoint()\n",
            "      (hook_A_bar): HookPoint()\n",
            "      (hook_B): HookPoint()\n",
            "      (hook_B_bar): HookPoint()\n",
            "      (hook_C): HookPoint()\n",
            "      (hook_h): InputDependentHookPoint()\n",
            "      (hook_y): HookPoint()\n",
            "      (hook_ssm_output): HookPoint()\n",
            "      (hook_after_skip): HookPoint()\n",
            "      (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      (hook_out_proj): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "  )\n",
            "  (norm): RMSNorm()\n",
            "  (hook_norm): HookPoint()\n",
            "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
            "  (hook_logits): HookPoint()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZfKT5aDOVHv"
      },
      "source": [
        "Let's start by generating some stories using the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TPXH2kSu3wSV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed it back to the model and keep predicting tokens:\n",
        "prompt = \"Once upon a time, there was a little girl named Lily. She lived in a big, happy little girl. On her big adventure,\"\n",
        "for i in range(2):\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    logits, activations = model.run_with_cache(tokens,\n",
        "                                               fast_ssm=True,\n",
        "                                               fast_conv=True,\n",
        "                                               warn_disabled_hooks=False\n",
        "                                               )\n",
        "    generated_text = tokenizer.batch_decode(logits.argmax(dim=-1)[0])\n",
        "    prompt += ' '.join(generated_text)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "WdKGGuWV1Ekh",
        "outputId": "75108de4-98d9-4098-a916-c7054aa76d3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there was a little girl named Lily. She lived in a big, happy little girl. On her big adventure, again  a  time ,  there  was  a  man  girl  named  Alice  who  She  was  in  a  small  house  big  house  house 's  She  the  little , ,  she again  a  time ,  there  was  a  man  girl  named  Alice  who  She  was  in  a  small  house  big  house  house 's  She  the  little , ,  she , \n",
            "  little little ,  she she  was was  a a  little little  named who    named    L ice . who    was    was    a    a    big    girl    in    girl    in    s    big    was    little          little   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsfJX-YpOVHv"
      },
      "source": [
        "### Spot checking model abilities with `transformer_lens.utils.test_prompt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TpmPoj7uOVHv",
        "outputId": "34b8d8bf-8ad7-4502-d6cd-6a0c9f5e08b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' little', ' girl', ' named', ' Lily', '.', ' She', ' lived', ' in', ' a', ' big', ',', ' happy', ' little', ' girl', '.', ' On', ' her', ' big', ' adventure', ',']\n",
            "Tokenized answer: [' Lily']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Performance on answer token:\n",
              "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m24.37\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.73\u001b[0m\u001b[1m% Token: | Lily|\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
              "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.37</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.73</span><span style=\"font-weight: bold\">% Token: | Lily|</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 0th token. Logit: 25.97 Prob: 63.23% Token: | she|\n",
            "Top 1th token. Logit: 24.37 Prob: 12.73% Token: | Lily|\n",
            "Top 2th token. Logit: 23.73 Prob:  6.74% Token: | the|\n",
            "Top 3th token. Logit: 23.00 Prob:  3.25% Token: | her|\n",
            "Top 4th token. Logit: 21.95 Prob:  1.14% Token: | a|\n",
            "Top 5th token. Logit: 21.90 Prob:  1.08% Token: | there|\n",
            "Top 6th token. Logit: 21.52 Prob:  0.74% Token: | they|\n",
            "Top 7th token. Logit: 21.24 Prob:  0.56% Token: | it|\n",
            "Top 8th token. Logit: 20.74 Prob:  0.34% Token: | he|\n",
            "Top 9th token. Logit: 20.61 Prob:  0.30% Token: | all|\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Lily'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Lily'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Test the model with a prompt\n",
        "test_prompt(\n",
        "    \"Once upon a time, there was a little girl named Lily. She lived in a big, happy little girl. On her big adventure,\",\n",
        "    \" Lily\",\n",
        "    model,\n",
        "    prepend_space_to_answer=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGzOvReDOVHv"
      },
      "source": [
        "In the output above, we see that the model assigns ~ 70% probability to \"she\" being the next token, and a 13% chance to \" Lily\" being the next token. Other names like Lucy or Anna are not highly ranked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH8YOZOzOVHv"
      },
      "source": [
        "### Exploring Model Capabilities with Log Probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50mqTBihOVHw"
      },
      "source": [
        "Looking at token ranking for a single prompt is interesting, but a much higher through way to understand models is to look at token log probs for all tokens in text. We can use the `circuits_vis` package to get a nice visualization where we can see tokenization, and hover to get the top5 tokens by log probability. Darker tokens are tokens where the model assigned a higher probability to the actual next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Tic0RCUpOVHw",
        "outputId": "5e679685-9cd2-41f5-a073-e205add087be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7a47fdf15ff0>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-ddf2701e-2ac8\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-ddf2701e-2ac8\",\n",
              "      TokenLogProbs,\n",
              "      {\"prompt\": [\"<|endoftext|>\", \"Hi\", \",\", \" how\", \" are\", \" you\", \" doing\", \" this\", \"?\", \" I\", \"'m\", \" really\", \" enjoying\", \" your\", \" posts\"], \"topKLogProbs\": [[-2.0294628143310547, -3.0690135955810547, -3.8244991302490234, -3.989736557006836, -4.26222038269043, -4.317106246948242, -4.328229904174805, -4.485235214233398, -4.554662704467773, -4.594663619995117], [-1.3927068710327148, -2.5150022506713867, -2.5160322189331055, -2.6099462509155273, -3.24735164642334, -3.2618398666381836, -3.554629325866699, -4.013983726501465, -4.080687522888184, -4.314108848571777], [-1.303799033164978, -1.4873889684677124, -3.50083589553833, -3.786411762237549, -3.9487576484680176, -4.133190631866455, -4.218555927276611, -4.295185565948486, -4.302296161651611, -4.901237964630127], [-0.6492132544517517, -2.6000266075134277, -2.6452231407165527, -2.8917746543884277, -3.3241782188415527, -3.5083975791931152, -3.8196158409118652, -4.136167049407959, -4.278619289398193, -4.584020137786865], [-0.14413581788539886, -3.399113893508911, -3.9703457355499268, -4.102113246917725, -4.324212551116943, -4.549630641937256, -5.142259120941162, -5.242311000823975, -5.377862453460693, -5.931859493255615], [-0.49467137455940247, -2.9921326637268066, -3.292328357696533, -3.5465798377990723, -3.6361184120178223, -3.727295398712158, -3.734976291656494, -3.8597970008850098, -4.2194085121154785, -4.3274922370910645], [-0.5796945691108704, -2.707700490951538, -2.898732900619507, -3.0418717861175537, -3.262746572494507, -3.9314591884613037, -4.104043960571289, -4.155637741088867, -4.71589469909668, -4.839509963989258], [-1.8530129194259644, -1.9254463911056519, -2.492478370666504, -2.714287757873535, -3.0006532669067383, -3.189457893371582, -3.54665470123291, -3.559250831604004, -3.8983240127563477, -4.161248207092285], [-1.49098801612854, -1.741995096206665, -3.386972665786743, -3.696485757827759, -4.069925308227539, -4.228109359741211, -4.241235733032227, -4.310155868530273, -4.394315719604492, -4.482927322387695], [-2.0374717712402344, -2.0487632751464844, -2.1472549438476562, -3.1236801147460938, -3.2557640075683594, -3.368824005126953, -3.699481964111328, -3.827117919921875, -3.9906387329101562, -4.044624328613281], [-2.7568933963775635, -3.3331987857818604, -3.366722345352173, -3.4078867435455322, -3.4121363162994385, -3.7332193851470947, -3.7455294132232666, -3.8044817447662354, -3.8317339420318604, -3.8841631412506104], [-1.9323383569717407, -2.273963451385498, -2.542534351348877, -2.7996325492858887, -3.1195874214172363, -3.2023062705993652, -3.319897174835205, -3.3871636390686035, -3.731433391571045, -4.039961338043213], [-1.674013376235962, -1.9695265293121338, -2.320780038833618, -2.3385069370269775, -3.2167418003082275, -3.638895273208618, -3.7556326389312744, -3.9974348545074463, -4.186563491821289, -4.362428665161133], [-2.3473637104034424, -2.7963955402374268, -3.28851056098938, -3.288884401321411, -3.5008060932159424, -4.3729143142700195, -4.476365089416504, -4.516667366027832, -4.5658769607543945, -4.670201301574707]], \"topKTokens\": [[\"Q\", \"The\", \"1\", \"A\", \"\\n\", \"In\", \"[\", \"This\", \"---\", \"/*\"], [\",\", \" everyone\", \" guys\", \" there\", \" all\", \"!\", \".\", \" everybody\", \" I\", \" and\"], [\" I\", \"\\n\", \" my\", \" i\", \" this\", \"I\", \" how\", \"\\n\\n\\n\", \" everyone\", \" My\"], [\" are\", \" can\", \" is\", \"'s\", \" do\", \" you\", \"\\u2019\", \"s\", \" may\", \"'re\"], [\" you\", \" ya\", \" u\", \" things\", \" y\", \" the\", \" your\", \" all\", \" people\", \" ye\"], [\"?\", \"!\", \" doing\", \" guys\", \",\", \" and\", \"\\n\", \".\", \" today\", \" ?\"], [\"?\", \"!\", \",\", \".\", \" today\", \" ?\", \"\\n\", \" and\", \"??\", \"?!\"], [\" morning\", \" week\", \" time\", \" weekend\", \" is\", \" year\", \"?\", \" month\", \" afternoon\", \" evening\"], [\"\\n\", \" I\", \"  \", \" It\", \" My\", \" What\", \" We\", \" You\", \" How\", \" Do\"], [\" have\", \"'m\", \" am\", \"'ve\", \" was\", \" just\", \" love\", \" need\", \" can\", \"\\u2019\"], [\" a\", \" so\", \" looking\", \" not\", \" in\", \" going\", \" having\", \" trying\", \" new\", \" just\"], [\" looking\", \" excited\", \" happy\", \" enjoying\", \" glad\", \" sorry\", \" interested\", \" impressed\", \" new\", \" curious\"], [\" the\", \" this\", \" my\", \" your\", \" it\", \" playing\", \" reading\", \" working\", \" watching\", \" all\"], [\" blog\", \" site\", \" work\", \" website\", \" new\", \" posts\", \" book\", \" post\", \" product\", \" videos\"]], \"correctTokenRank\": [172, 0, 6, 0, 0, 2, 10, 6, 1, 1, 18, 3, 3, 5], \"correctTokenLogProb\": [-7.331357955932617, -1.3927068710327148, -4.218555927276611, -0.6492132544517517, -0.14413581788539886, -3.292328357696533, -5.031167984008789, -3.54665470123291, -1.741995096206665, -2.0487632751464844, -4.5759735107421875, -2.7996325492858887, -2.3385069370269775, -4.3729143142700195]}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Let's make a longer prompt and see the log probabilities of the tokens\n",
        "example_prompt = \"\"\"Hi, how are you doing this? I'm really enjoying your posts\"\"\"\n",
        "logits, cache = model.run_with_cache(example_prompt)\n",
        "cv.logits.token_log_probs(\n",
        "    model.to_tokens(example_prompt),\n",
        "    model(example_prompt)[0].log_softmax(dim=-1),\n",
        "    model.to_string,\n",
        ")\n",
        "# hover on the output to see the result."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Code used to remove the \"rare freq direction\", the shared direction among the ultra low frequency features.\n",
        "# I experimented with removing it and retraining the autoencoder.\n",
        "if cfg[\"remove_rare_dir\"]:\n",
        "    rare_freq_dir = torch.load(\"rare_freq_dir.pt\")\n",
        "    rare_freq_dir.requires_grad = False\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Training cfg:\n",
        "cfg = {\n",
        "    # Data parameters\n",
        "    \"num_tokens\": int(1e5),  # Total number of tokens to use\n",
        "    \"batch_size\": 32,  # Batch size for training\n",
        "\n",
        "    # Model parameters\n",
        "    \"act_name\": \"hook_norm\",  # Name of the activation to extract\n",
        "    \"dict_size\": 1536,#3072,\n",
        "    \"l1_coeff\": 3e-4,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.99,\n",
        "    \"dict_mult\": 32,\n",
        "    \"seq_len\": 128,\n",
        "    \"remove_rare_dir\": False,\n",
        "    \"device\": \"cuda:0\",\n",
        "    \"enc_dtype\": \"fp32\",\n",
        "    \"seed\": 16,\n",
        "    \"act_size\": 768,\n",
        "    \"model_batch_size\": 32,\n",
        "\n",
        "    # Training parameters\n",
        "    \"num_epochs\": 5,  # Number of epochs to train\n",
        "    \"lr\": 1e-3,  # Learning rate\n",
        "    \"beta1\": 0.9,  # Adam optimizer beta1\n",
        "    \"beta2\": 0.999,  # Adam optimizer beta2\n",
        "\n",
        "    # Regularization\n",
        "    \"l1_weight\": 1e-5,  # L1 regularization weight\n",
        "    \"l2_weight\": 1e-5,  # L2 regularization weight\n",
        "\n",
        "    # Logging and checkpointing\n",
        "    \"log_every\": 50,  # Log every N batches\n",
        "    \"eval_every\": 100,  # Evaluate reconstruction every N batches\n",
        "    \"recons_every\": 500,  # Reconstruct every N batches\n",
        "    \"save_every\": 500,  # Save model every N batches\n",
        "    \"reset_freq_threshold\": 10**(-5.5),  # Frequency threshold for resetting neurons\n",
        "\n",
        "    # Wandb configuration\n",
        "    \"wandb_project\": \"mamba_autoencoder\",\n",
        "    \"wandb_name\": None,\n",
        "\n",
        "    # Model specific (you might need to adjust these)\n",
        "    \"encoder_hidden_sizes\": [512, 256],  # Hidden layer sizes for encoder\n",
        "    \"decoder_hidden_sizes\": [256, 512],  # Hidden layer sizes for decoder\n",
        "    \"latent_dim\": 64,  # Dimension of the latent space\n",
        "}"
      ],
      "metadata": {
        "id": "NjprIIMd6H89"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation:"
      ],
      "metadata": {
        "id": "bNEPo2CFHs8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def shuffle_data(all_tokens):\n",
        "    print(\"Shuffled data\")\n",
        "    return all_tokens[torch.randperm(all_tokens.shape[0])]\n",
        "\n",
        "num_tokens=cfg['num_tokens']\n",
        "loading_data_first_time = False\n",
        "if loading_data_first_time:\n",
        "    data = load_dataset(\"NeelNanda/c4-code-tokenized-2b\", split=\"train\", cache_dir=\"/workspace/cache/\")\n",
        "    data.save_to_disk(\"/workspace/data/c4_code_tokenized_2b.hf\")\n",
        "    data.set_format(type=\"torch\", columns=[\"tokens\"])\n",
        "    limited_tokens = data[\"tokens\"][:num_tokens]\n",
        "    print(limited_tokens.shape)\n",
        "\n",
        "\n",
        "    limited_tokens_reshaped = einops.rearrange(limited_tokens, \"batch (x seq_len) -> (batch x) seq_len\", x=8, seq_len=128)\n",
        "    limited_tokens_reshaped[:, 0] = model.tokenizer.bos_token_id\n",
        "    limited_tokens_reshaped = limited_tokens_reshaped[torch.randperm(limited_tokens_reshaped.shape[0])]\n",
        "    torch.save(limited_tokens_reshaped, \"/workspace/data/c4_code_2e5_tokens_reshaped.pt\")\n",
        "\n",
        "    print(f\"Saved {limited_tokens_reshaped.shape[0]}\")\n",
        "else:\n",
        "    #data = datasets.load_from_disk(\"/workspace/data/c4_code_tokenized_2b.hf\")\n",
        "    all_tokens = torch.load(\"/workspace/data/c4_code_2e5_limited_tokens_reshaped.pt\")\n",
        "    all_tokens = shuffle_data(all_tokens)"
      ],
      "metadata": {
        "id": "GtNuBA2dUMhu",
        "outputId": "cfcbd667-369f-49f1-abf3-431566457a5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = shuffle_data(all_tokens[:num_tokens,])\n",
        "print(all_tokens.shape)\n",
        "print(all_tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRgTMLRH3FQn",
        "outputId": "6ce11c07-8b88-411b-8f0c-9db456848e23"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled data\n",
            "torch.Size([100000, 128])\n",
            "tensor([    0,    65, 33653,    65, 42524,    28, 21315,  4873,    65,  1636,\n",
            "           75,    65,  9097,    16, 15983,    65,    88,  4753,    10,    88,\n",
            "        13043,    65,   302,    14,  2176,    11, 23125,  1076, 11475, 16808,\n",
            "        11916, 40056,  4078,  4021, 32048,  1525, 26132, 14769,  3451, 11916,\n",
            "        16808,   804,  5166,    65,  6271,    65,  5238,    10,  1262,    14,\n",
            "         8758,    31,    18,  2192,  5292,   603,  8758,  2224,   470,    28,\n",
            "         7959,  1827,    16,  6271,    65,  5238,    65, 11628,   426,   338,\n",
            "         7959,  1076, 23125,  6191,   426, 31681,    10,  3176,    31,  4874,\n",
            "           10,  1262,    16,  6271,    65, 10649,    65,  3434,  1210, 23125,\n",
            "          804,   279,    65, 14837,    65,  6271,    65,  5238,    10, 10649,\n",
            "         2192,  7959,   278,  5015,    65,  9097,   426,  1827,    16,  6271,\n",
            "           65,  1636,    75,    65,  3434,    61, 10649,    61,   298,  8092,\n",
            "           65,  3361,    16, 29411,    65, 33653,    65, 15098])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "g9cpVo2g5zzc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenDataset(Dataset):\n",
        "    def __init__(self, tokens, max_tokens=int(2e5)):\n",
        "        self.tokens = tokens[:max_tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokens[idx]\n",
        "\n",
        "# Load your data\n",
        "all_tokens = all_tokens.to(device)  # Move the data to the appropriate device\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TokenDataset(all_tokens)\n",
        "dataloader = DataLoader(dataset, batch_size=cfg[\"batch_size\"], shuffle=True)"
      ],
      "metadata": {
        "id": "ziLNsKr653TB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Class"
      ],
      "metadata": {
        "id": "EmRGmsqiooH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = Path(\"/workspace/checkpoints\")\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d_hidden = cfg[\"dict_size\"]\n",
        "        l1_coeff = cfg[\"l1_coeff\"]\n",
        "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
        "        torch.manual_seed(cfg[\"seed\"])\n",
        "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(cfg[\"act_size\"], d_hidden, dtype=dtype)))\n",
        "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, cfg[\"act_size\"], dtype=dtype)))\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
        "\n",
        "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.l1_coeff = l1_coeff\n",
        "\n",
        "        self.to(cfg[\"device\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"x.shape: {x.shape}\")\n",
        "        x_cent = x - self.b_dec\n",
        "        #print(f\"x_cent.shape: {x_cent.shape}\")\n",
        "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
        "        #print(f\"acts.shape: {acts.shape}\")\n",
        "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
        "        #print(f\"x_reconstruct.shape: {x_reconstruct.shape}\")\n",
        "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean()\n",
        "        #print(f\"l2_loss: {l2_loss}\")\n",
        "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
        "        #print(f\"l1_loss: {l1_loss}\")\n",
        "        loss = l2_loss + l1_loss\n",
        "        #print(f\"loss: {loss}\")\n",
        "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def make_decoder_weights_and_grad_unit_norm(self):\n",
        "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
        "        self.W_dec.grad -= W_dec_grad_proj\n",
        "        # Bugfix(?) for ensuring W_dec retains unit norm, this was not there when I trained my original autoencoders.\n",
        "        self.W_dec.data = W_dec_normed\n",
        "\n",
        "    def get_version(self):\n",
        "        version_list = [int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)]\n",
        "        if len(version_list):\n",
        "            return 1+max(version_list)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def save(self):\n",
        "        version = self.get_version()\n",
        "        torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
        "        with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
        "            json.dump(cfg, f)\n",
        "        print(\"Saved as version\", version)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, version):\n",
        "        cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
        "        pprint.pprint(cfg)\n",
        "        self = cls(cfg=cfg)\n",
        "        self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
        "        return self\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_hf(cls, version):\n",
        "        \"\"\"\n",
        "        Loads the saved autoencoder from HuggingFace.\n",
        "\n",
        "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
        "\n",
        "        version 25 is the final checkpoint of the first autoencoder run,\n",
        "        version 47 is the final checkpoint of the second autoencoder run.\n",
        "        \"\"\"\n",
        "        if version==\"run1\":\n",
        "            version = 25\n",
        "        elif version==\"run2\":\n",
        "            version = 47\n",
        "\n",
        "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
        "        pprint.pprint(cfg)\n",
        "        self = cls(cfg=cfg)\n",
        "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
        "        return self"
      ],
      "metadata": {
        "id": "MKnPU7TqOZQ9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities for Training"
      ],
      "metadata": {
        "id": "wUFJl2D74B8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = cfg[\"seed\"]\n",
        "GENERATOR = torch.manual_seed(SEED)\n",
        "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "n_layers = model.cfg.n_layers\n",
        "d_model = model.cfg.d_model\n",
        "# %%\n",
        "@torch.no_grad()\n",
        "def get_sub_acts(tokens, batch_size=1024):\n",
        "    outputs, cache = model.run_with_cache(tokens,\n",
        "                                          fast_ssm=True,\n",
        "                                          fast_conv=True,\n",
        "                                          warn_disabled_hooks=False,\n",
        "                                          names_filter=cfg[\"act_name\"])\n",
        "    acts = cache[cfg[\"act_name\"]]\n",
        "    del outputs, cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    acts = acts.reshape(-1, acts.shape[-1])\n",
        "    subsample = torch.randperm(acts.shape[0], generator=GENERATOR)[:batch_size]\n",
        "    subsampled_acts = acts[subsample, :]\n",
        "    return subsampled_acts, acts\n",
        "# sub, acts = get_acts(torch.arange(20).reshape(2, 10), batch_size=3)\n",
        "# sub.shape, acts.shape\n",
        "# %%\n",
        "@torch.no_grad()\n",
        "def get_acts(tokens, batch_size=1024):\n",
        "    outputs, cache = model.run_with_cache(tokens,\n",
        "                                          fast_ssm=True,\n",
        "                                          fast_conv=True,\n",
        "                                          warn_disabled_hooks=False,\n",
        "                                          names_filter=cfg[\"act_name\"])\n",
        "    acts = cache[cfg[\"act_name\"]]\n",
        "    del outputs, cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    acts = acts.reshape(-1, acts.shape[-1])\n",
        "    return acts"
      ],
      "metadata": {
        "id": "zCe2wa85S_2E"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replacement_hook(pre_linear, hook, encoder):\n",
        "    pre_linear_reconstr = encoder(pre_linear)[1]\n",
        "    return pre_linear_reconstr\n",
        "\n",
        "def mean_ablate_hook(pre_linear, hook):\n",
        "    pre_linear[:] = pre_linear.mean([0, 1])\n",
        "    return pre_linear\n",
        "\n",
        "def zero_ablate_hook(pre_linear, hook):\n",
        "    pre_linear[:] = 0.\n",
        "    return pre_linear\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_recons_loss(num_batches=5, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    loss_list = []\n",
        "    for i in range(num_batches):\n",
        "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
        "        loss = model(tokens, return_type=\"loss\")\n",
        "        recons_loss = model.run_with_hooks(tokens, return_type=\"loss\",\n",
        "                                           fast_ssm=True,\n",
        "                                           fast_conv=True,\n",
        "                                           fwd_hooks=[(cfg[\"act_name\"], partial(replacement_hook, encoder=local_encoder))])\n",
        "        # mean_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(cfg[\"act_name\"], mean_ablate_hook)])\n",
        "        zero_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\",\n",
        "                                             fast_ssm=True,\n",
        "                                             fast_conv=True,\n",
        "                                             fwd_hooks=[(cfg[\"act_name\"], zero_ablate_hook)])\n",
        "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
        "    losses = torch.tensor(loss_list)\n",
        "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
        "\n",
        "    print(loss, recons_loss, zero_abl_loss)\n",
        "    score = ((zero_abl_loss - recons_loss)/(zero_abl_loss - loss))\n",
        "    print(f\"{score:.2%}\")\n",
        "    # print(f\"{((zero_abl_loss - mean_abl_loss)/(zero_abl_loss - loss)).item():.2%}\")\n",
        "    return score, loss, recons_loss, zero_abl_loss\n",
        "# print(get_recons_loss())\n",
        "\n",
        "# %%\n",
        "# Frequency\n",
        "@torch.no_grad()\n",
        "def get_freqs(num_batches=25, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).to(cfg[\"device\"])\n",
        "    total = 0\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
        "\n",
        "        _, cache = model.run_with_cache(tokens, names_filter=cfg[\"act_name\"], fast_ssm=True, fast_conv=True, warn_disabled_hooks=False)\n",
        "        acts = cache[cfg[\"act_name\"]]\n",
        "        acts = acts.reshape(-1, cfg[\"act_size\"])\n",
        "\n",
        "        hidden = local_encoder(acts)[2]\n",
        "\n",
        "        act_freq_scores += (hidden > 0).sum(0)\n",
        "        total+=hidden.shape[0]\n",
        "    act_freq_scores /= total\n",
        "    num_dead = (act_freq_scores==0).float().mean()\n",
        "    print(\"Num dead\", num_dead)\n",
        "    return act_freq_scores\n",
        "# %%\n",
        "@torch.no_grad()\n",
        "def re_init(indices, encoder):\n",
        "    new_W_enc = (torch.nn.init.kaiming_uniform_(torch.zeros_like(encoder.W_enc)))\n",
        "    new_W_dec = (torch.nn.init.kaiming_uniform_(torch.zeros_like(encoder.W_dec)))\n",
        "    new_b_enc = (torch.zeros_like(encoder.b_enc))\n",
        "    print(new_W_dec.shape, new_W_enc.shape, new_b_enc.shape)\n",
        "    encoder.W_enc.data[:, indices] = new_W_enc[:, indices]\n",
        "    encoder.W_dec.data[indices, :] = new_W_dec[indices, :]\n",
        "    encoder.b_enc.data[indices] = new_b_enc[indices]"
      ],
      "metadata": {
        "id": "lve-_edPVmHS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lim_tokens = all_tokens[:32]\n",
        "print(lim_tokens.shape)\n",
        "\n",
        "outputs, MambaActs = model.run_with_cache(\n",
        "                                        lim_tokens,\n",
        "                                        fast_ssm=True,\n",
        "                                        fast_conv=True,\n",
        "                                        warn_disabled_hooks=False\n",
        "                                        )\n",
        "\n",
        "acts = MambaActs[cfg[\"act_name\"]]\n",
        "print(acts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "TIpc0M3Sa3p7",
        "outputId": "0d342c87-bfd9-43ca-8f29-c0227a1c0f84"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 81013 has 7.59 GiB memory in use. Process 80106 has 7.16 GiB memory in use. Of the allocated memory 7.37 GiB is allocated by PyTorch, and 90.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-577517df61fa>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlim_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m outputs, MambaActs = model.run_with_cache(\n\u001b[0m\u001b[1;32m      5\u001b[0m                                         \u001b[0mlim_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                         \u001b[0mfast_ssm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHookedRootModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \"\"\"\n\u001b[0;32m--> 643\u001b[0;31m         out, cache_dict = super().run_with_cache(\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_lens/input_dependent_hooks.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dependent_hooks_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwd_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbwd_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup_all_input_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetup_all_input_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 res = super().run_with_cache(\n\u001b[0m\u001b[1;32m    341\u001b[0m                     \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mnames_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0mclear_contexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_contexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         ):\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincl_bwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_lens/HookedMamba.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, stop_at_layer, only_use_these_layers, fast_conv, fast_ssm, warn_disabled_hooks)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0monly_use_these_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0;31m# [B,L,D]         [B,L,D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresid\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_ssm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast_ssm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_disabled_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_disabled_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;31m# we stop early, just return the resid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mamba_lens/HookedMamba.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid, fast_conv, fast_ssm, warn_disabled_hooks)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;31m# [B,L,D]         [E->D]   [B,L,E]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \u001b[0my_out\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_after_skip\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# no bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m         \u001b[0my_out\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_out_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B,L,D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 81013 has 7.59 GiB memory in use. Process 80106 has 7.16 GiB memory in use. Of the allocated memory 7.37 GiB is allocated by PyTorch, and 90.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del outputs, MambaActs\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "LZUI8z0ubBYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Run"
      ],
      "metadata": {
        "id": "udkJM87H4DHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "DTYPES = {\n",
        "    \"fp32\": torch.float32,\n",
        "    \"fp16\": torch.float16,\n",
        "    \"bf16\": torch.bfloat16\n",
        "} # Add this line to define DTYPES\n",
        "\n",
        "encoder = AutoEncoder(cfg)"
      ],
      "metadata": {
        "id": "cezNrtJCK54p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wandb Args:\n",
        "wandb_args = {\n",
        "  \"wandb_project\": \"mamba-sae\",\n",
        "  \"wandb_name\": None,\n",
        "}\n",
        "\n",
        "try:\n",
        "    wandb.init(\n",
        "        project=wandb_args[\"wandb_project\"],\n",
        "        name=wandb_args[\"wandb_name\"],\n",
        "    )\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    print(f\"Number of tokens: {cfg['num_tokens']}\")\n",
        "    print(f\"Batch size: {cfg['batch_size']}\")\n",
        "    print(f\"Number of batches: {num_batches}\")\n",
        "\n",
        "    encoder_optim = torch.optim.Adam(encoder.parameters(), lr=cfg[\"lr\"], betas=(cfg[\"beta1\"], cfg[\"beta2\"]))\n",
        "    recons_scores = []\n",
        "    act_freq_scores_list = []\n",
        "\n",
        "    for epoch in range(cfg[\"num_epochs\"]):  # Add an outer loop for epochs if needed\n",
        "        for i, tokens in enumerate(tqdm(dataloader)):\n",
        "            tokens = tokens.to(device)\n",
        "\n",
        "            #sub_acts = get_sub_acts(tokens, 16)[1]\n",
        "            #acts = sub_acts\n",
        "            outputs, cache = model.run_with_cache(tokens,\n",
        "                                          fast_ssm=True,\n",
        "                                          fast_conv=True,\n",
        "                                          warn_disabled_hooks=False,\n",
        "                                          names_filter=cfg[\"act_name\"])\n",
        "            acts = cache[cfg[\"act_name\"]]\n",
        "            del outputs, cache\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            acts = acts.reshape(-1, cfg[\"act_size\"])\n",
        "\n",
        "            loss, x_reconstruct, mid_acts, l2_loss, l1_loss = encoder(acts)\n",
        "            loss.backward()\n",
        "\n",
        "            encoder.make_decoder_weights_and_grad_unit_norm()\n",
        "            encoder_optim.step()\n",
        "            encoder_optim.zero_grad()\n",
        "\n",
        "            loss_dict = {\"loss\": loss.item(), \"l2_loss\": l2_loss.item(), \"l1_loss\": l1_loss.item()}\n",
        "            del loss, x_reconstruct, mid_acts, l2_loss, l1_loss, acts\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "            print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "            print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
        "            \"\"\"\n",
        "\n",
        "            if (i + 1) % cfg[\"log_every\"] == 0:\n",
        "                wandb.log(loss_dict)\n",
        "                print(loss_dict)\n",
        "\n",
        "            if (i + 1) % cfg[\"recons_every\"] == 0:\n",
        "                x = get_recons_loss(local_encoder=encoder)\n",
        "                print(\"Reconstruction:\", x)\n",
        "                recons_scores.append(x[0])\n",
        "                freqs = get_freqs(5, local_encoder=encoder)\n",
        "                act_freq_scores_list.append(freqs)\n",
        "                wandb.log({\n",
        "                    \"recons_score\": x[0],\n",
        "                    \"dead\": (freqs==0).float().mean().item(),\n",
        "                    \"below_1e-6\": (freqs<1e-6).float().mean().item(),\n",
        "                    \"below_1e-5\": (freqs<1e-5).float().mean().item(),\n",
        "                })\n",
        "\n",
        "            if (i + 1) % cfg[\"save_every\"] == 0:\n",
        "                encoder.save()\n",
        "                wandb.log({\"reset_neurons\": 0.0})\n",
        "                freqs = get_freqs(50, local_encoder=encoder)\n",
        "                to_be_reset = (freqs < cfg[\"reset_freq_threshold\"])\n",
        "                print(\"Resetting neurons!\", to_be_reset.sum())\n",
        "                re_init(to_be_reset, encoder)\n",
        "\n",
        "finally:\n",
        "    encoder.save()"
      ],
      "metadata": {
        "id": "7jY4fo3uVooQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder)"
      ],
      "metadata": {
        "id": "axop3i_-G7mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load(72)"
      ],
      "metadata": {
        "id": "Kd1VY4D5G9KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # This will prompt for your auth token if you haven't saved it before"
      ],
      "metadata": {
        "id": "do0s-3gzHKmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, Repository"
      ],
      "metadata": {
        "id": "LnHkO4bvHj5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"amantimalsina/mamba-sae\"\n",
        "api = HfApi()\n",
        "repo_url = api.create_repo(repo_name, exist_ok=True)\n",
        "\n",
        "# 4. Clone the empty repository\n",
        "repo = Repository(local_dir=\"./my_model_repo\", clone_from=repo_url)"
      ],
      "metadata": {
        "id": "-AQbGtF7HtTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "version = 72\n",
        "import shutil\n",
        "shutil.copy(SAVE_DIR/(str(version)+\".pt\"), f\"./my_model_repo/{version}.pt\")\n",
        "shutil.copy(SAVE_DIR/(str(version)+\"_cfg.json\"), f\"./my_model_repo/{version}_cfg.json\")\n",
        "\n",
        "# 6. Create a simple README\n",
        "with open(\"./my_model_repo/README.md\", \"w\") as f:\n",
        "    f.write(f\"# AutoEncoder Model\\n\\nThis is a custom AutoEncoder model for Mamba-130m. Version: {version}\")"
      ],
      "metadata": {
        "id": "QB8hmvN_Inwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 6. Push the changes to the Hub\n",
        "repo.git_add()\n",
        "repo.git_commit(\"Added encoder for ssm_output\")\n",
        "repo.git_push()\n",
        "\n",
        "print(f\"Model saved to https://huggingface.co/{repo_name}\")"
      ],
      "metadata": {
        "id": "1KdF8ZS5IHzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QgnXY3UJGFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}